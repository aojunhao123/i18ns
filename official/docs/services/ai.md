# 接入兼容 OpenAI API 接口的 AI 模型

> 原作者：萧萧然

## 基本步骤

1. 打开沉浸式翻译插件的[设置页 - 翻译服务页](https://dash.immersivetranslate.com/#services)
2. 滑动至翻译服务底部，点击文字`添加兼容 OpenAI 接口的自定义 AI 翻译服务？`。
3. 为你自定义接入的模型服务设置一个自定义服务名称，以便在使用时区分。例如，你想使用的是 Groq 平台的 Mixtral 模型，使用的 Groq 账号是账号 A，那么你可以设置为`Groq-Mixtral-A账号`。
4. 接下来，按照类似配置 OpenAI GPT 系列模型一样的配置方式，填写你的 API Key、模型名、请求 URL 地址等信息。
5. 之后可依据个人所需配置自定义提示词和 temperature 参数，如果不懂这些是干什么的，请保持默认不变。之后点击`点此测试服务`，系统会自动保存。

## 配置说明

### 自定义 API 接口地址

你的自定义 AI 模型服务的 API 接口完整请求地址，该接口地址要求兼容 OpenAI 接口格式，通常会以`chat/completions`等路径结尾，例如`https://api.groq.com/openai/v1/chat/completions`。注意需要确保你的主机浏览器与请求地址之间通信良好，例如当你将 Ollama 配置在家里的局域网内某机器上，但是浏览器使用了全局代理，流量全部发送给了代理服务器，包括局域网请求，那么你的请求就无法完成，无法通过服务测试。

### APIKEY

通常是一个长字符串，用于验证你的请求身份，可以在你的 AI 模型服务平台的账号密钥设置页中找到，不同平台使用的格式是不同的，错误、欠费的 APIKEY 无法测试成功。例如`sk-O6FclmECYFJyTB664b6681D06eEb47919b8570Be1fA2B434`。

### 模型

准确来说，指的是请求时发送的模型名字符串。不同平台的模型名格式不同，不同模型名代表了不同模型选择，因此计费和速率控制等都不同，请严格依据平台文档选择并填入，不要看着选择列表里某个模型名看着像你想要的模型就随便选一个，更常见的情况下，你需要勾选`输入自定义模型名称`来精确填写你想要使用的模型名，尤其是对于那些想要精准控制模型版本的人。例如 ollama 中的`phi3:14b-medium-4k-instruct-q4_0`代表了微软的开源模型 Phi3 的中杯版本（14B 参数，Medium 大小）上下文窗口 4K 大小、指令微调后 Q4_0 量化方法的版本。注意不要误使用 base 模型或者非 instruct/chat 模型，这些模型没有专门针对对话数据进行训练，指令跟随效果不佳，很有可能无法正常返回译文。

### 每秒最大请求数

翻译插件在将文本发送给 AI 模型接口进行翻译时，会根据这个参数来控制发送请求的速率，以避免超出 AI 模型服务的速率限制。请求数超过该限制时会进入排队状态，直到下一秒钟开始。较大的请求数翻译时具有较大的并发，通常可以更快地完成文字较多的网页的翻译，但是也很有可能被某些平台视为滥用 API（例如 Google）。较小的请求数或许可以避免速率受限，但翻译速度会慢一些。计算合适的请求数需要查看平台的限速文档，通常是每分钟或每秒的请求数/查询数（RPM/QPS），例如百度 ERNIE-Lite-8K 模型官方文档注明的速率限制是 300RPM，300K TPM，代表每分钟可接受 300 次请求，请求的输入输出文本词元数最大为 300K。300RPM/60=5QPS，也就是说你理论上可以将插件的每秒最大请求数设置为 5，肯定不会超标。但是实际上个人使用时，插件几乎不可能在一分钟内连续以最大速率发送请求，因此可以适当提高这个数值，例如设置为 8，如果之后使用时遇到了限速错误，再适当调低。对于那些限速严重的平台，建议设置为最小值 1。

### 你可以指定一位 AI 专家来提供翻译策略

当你在 AI 专家 Tab 里安装过 AI 专家后，才会出现该选项，你可以通过选择不同的“专家”来选择不同的提示词进行翻译，如果你不懂这个是干什么的，请不要配置专家，或者保持默认的`通用`不变。需要注意的是，不同的专家（提示词）对不同的模型（甚至不同的版本上）可能会产生不同的效果，例如在 GPT-3.5 模型上效果良好的提示词在 Gemini 模型上效果可能就一般（例如产生无法解析的 YAML 译文格式等），这与不同模型具有不同的训练语料及 SFT 过程有关。

### 每次请求最大文本长度

指的是每次请求最大字符数（而非 NLP 模型采用的 Token 数/词元数）。此选项设置得太大（例如 2000）会导致返回一次译文的过程变慢，因为每次发送翻译请求得到结果需要 AI 模型给出较长的输出。同时设置得过大也有可能导致模型输出的译文丢失信息，或者输出无法解析或错误的译文格式（这样会导致插件在解析译文时顺序出错，导致译文错误地堆叠在一起）。但是设置的较大的好处是 AI 模型在翻译时（规模参数较大的模型）可以充分参考上下文，译文会更流畅和一致一些，对具有关联和语境的长文翻译效果更好。此选项设置得太小（例如 200）会导致插件在翻译时几乎不采用多段合并一起翻译，而更接近一段一段甚至是一句一句单独翻译，模型在缺少语境和上下文的情况下译文可能不够流畅和一致，但是不容易出现信息丢失、错误格式等情况，翻译速度在每秒最大请求数设置得较大的情况下会有一定提升。因此可以尝试调整该选项来优化速度和译文效果。对于需要快速翻译的网页浏览场景，可以调小到小于 1000，对于小说翻译等需要流畅翻译的场景可以调大，建议依据不同模型和场景在 500-2000 之内浮动。（BTW，如果你的自定义提示词较长，例如精心配置的专家提示词，调高这一设置还可以减少请求次数，帮你节省提示词占用的模型输入费用）

### 每次请求最大段落数

每次发送给翻译服务的段落数量，如果段落数量过多，自然会提升每次请求的原文长度，从而导致类似调高“每次请求最大文本长度”的效果，导致译文返回得较慢或者更容易出错，但是好处是译文在长文翻译时更流畅和一致。如果你设置为 1，插件不会采用 Multiple Prompt 来翻译，而是直接使用（单段翻译使用的）Prompt 来翻译，这样做可以使译文不会出现与原文对齐不一致的错误，但是对于参数较小、指令跟随较差的模型，容易产生额外的解释（例如只需要翻译一个单词，产生了额外的解释信息）。关于此项配置，对于规模较大、指令跟随效果较好的模型，可以适当调高（例如 GPT 系列模型可以调到 10 以上）；对于模型参数较小、指令跟随效果较差的模型，可以适当调低。但是更合适的做法是不要用质量较差的模型。如果你发现调高这个配置后，译文出现信息丢失、译文堆叠或者位置错误的情况，请调低这个配置。

### 每次字幕请求最大段落数

同每次请求最大段落数类似。

### temperature

采样发散度，范围[0, 2.0]（注意有的平台不能设置为 0，最小为 0.01）, 值越小，生成的内容越固定。当取 0 时，模型生成时几乎总是会选取概率最大的 Token（词元）。越低的采样发散度模型将越倾向于使用机翻风格逐句翻译，越高的采样发散度模型的译文将越随机，可能导致译文丢失部分信息，也有可能会给出更流畅的译文。文学翻译场景可以适当调高该参数，但是不建议超过 1.0，尤其是一些参数较小的模型，容易导致输出错误 YAML 格式的译文。

### System Prompt, Prompt, Multiple Prompt, Subtitle Prompt

你确定你需要调整提示词吗？如果你不懂这些提示词是干什么的，请不要修改，保持默认。如果你懂如何进行提示词调优，那么无需再多做额外的解释。只需提醒这些点：

- System Prompt 不一定每个模型都会遵守，有的模型甚至没有 System Prompt 参数，输入了也没用。
- Prompt 是单段翻译时使用的提示词，如果你设置了每次请求最大段落数为 1，那么插件会使用 Prompt 来翻译，否则会使用 Multiple Prompt 来翻译。
- Multiple Prompt 是多段翻译时使用的提示词，如果你设置了每次请求最大段落数大于 1，那么插件会使用 Multiple Prompt 来翻译，除非一段文本的长度超过了每次请求最大文本长度。
- 注意 `{{imt_trans_field}}` 、`{{to}}`、`{{yaml}}` 等必要的占位符需要保留。`{{yaml}}`占位符会被具体填充什么内容，可使用浏览器 F12 查看插件翻译时请求的 Payload。
- 提示词调优建议使用 API 和编程工具直接在“纯净模式”下来做，不建议直接使用官网聊天助手或者第三方聊天程序，因为其可能有预置提示词，或者改变了相关参数（如 Temperature 等），导致对调优过程的干扰。
- 沉浸式翻译暂时还不支持多轮对话提示词，只支持单轮对话请求的结果的 yaml 解析，因此如果你需要通过 Few-Shot（使用例子/少样本提示）、输出中间思考过程等方法来提升翻译质量，需要把例子放在单个提示词里，并通过`step1-step2-text`的样例结构构建最终的 yaml 格式，其中 text 是插件最终解析的译文。一个结合了 Few-Shot 和 CoT 的简陋例子如下：

```text
你是一个意译专家，对于给定的译文，你先进行初次翻译，而后依据全文进行二次润色，最后再给出最终意译译文。
要求的输出格式：你会先使用 first 路径给出初次翻译结果，再使用同输入相同的 text 路径给出结合全文的二次意译结果。

注意：
在正式输出意译结果前，在初次译文后，你需要用注释写明 2 个内容：
1. 上述初次翻译结果是否信、达、雅？你可以在这里的注释中指出哪里可以改进。
2. 初次翻译，一共输出了多少个 id？原文中有多少个 id？在此判断是否一致，如果一致则继续翻译。如果不一致，在注释里输出这句话：`初次翻译输出的id数与原文中的id数xx个不同，接下来的最终意译结果我会严格按照原文的id数（xx个）来匹配原文-译文输出。`

具体格式要求见__示例__。

__示例__
输入原文例子 1：
<yaml input>
- id:1
  text: "You are a translation expert."
- id:2
  text: "You will first translate the given text, then polish it, and finally give the final translation."
- id:3
  text: "You will follow the steps below to complete the translation."
</yaml>

回复译文例子 1：
<yaml output>
- first:
    - id:1
      text: "你是一个翻译专家。"
    - id:2
      text: "你将首先翻译给定的文本，然后进行润色，最后给出最终翻译。您将按照以下步骤完成翻译。"
# 上述初次翻译结果基本达到了信、达、雅的要求，但还可以进行一些改进：
# 改进 1：....
# 改进 2：....
# 初次翻译一共输出了 2 个 id，原文中有 3 个 id，不一致，接下来的最终意译结果我会严格按照原文的 id 数（3 个）来匹配原文 - 译文输出。
- id:1
  text: "你是一个精通翻译的专家。"
- id:2
  text: "你会首先初译给定的文本，然后进行润色，最后给出最终完美的意译。"
- id:3
  text: "你将按照以下指定的步骤来完成翻译。"
</yaml>

输入原文例子 2：...
回复译文例子 2：...

__待翻译文本__
现在，请将下面的文本翻译成{{to}}，只需回复 YAML 格式文本：
{{yaml}}
```

## 不同场景下的配置建议

- **需要快速翻译的网页浏览场景**：使用生成速度较快的模型（例如 Gemini-1.5-Flash/Claude3-haiku/GPT-3.5-Turbo 等）、设置较高的每秒最大请求数（例如 10）、设置较小的每次请求最大文本长度（例如 500）、设置较小的每次请求最大段落数（例如 1-5，如果翻译的场景中段落较短可以略高）。提示词缩短，不要采用输出中间结果的提示策略（如“意译专家”）。
- **需要翻译质量更好的小说翻译场景**：使用效果较好的模型（例如 GPT-4-Turbo/Claude3-Opus/Gemini-1.5-Pro），设置较大的每次请求最大文本长度（例如 2000）、设置合适的每次请求最大段落数（例如 5-15）增大上下文供模型翻译参考。提示词设置为专家提示词或自定义调优后的提示词，例如“意译专家”，增加了特定翻译风格示例的提示词等，可以适当调高 temperature 参数，例如 0.7。

## 具体平台接入必填参数举例

### Ollama 本地部署开源模型

- 安装配置并启动 Ollama
  1. 从[官网](https://ollama.com/)下载安装 Ollama
  2. 设置允许跨域并启动
  - macOS：命令行执行 `launchctl setenv OLLAMA_ORIGINS "*"`，再启动 App。
  - Windows：控制面板 - 系统属性 - 环境变量 - 用户环境变量新建 2 个环境变量：变量名`OLLAMA_HOST`变量值`0.0.0.0`，变量名`OLLAMA_ORIGINS`变量值`*`，再启动 App。
  - Linux：命令行执行 `OLLAMA_ORIGINS="*" ollama serve`。
- 翻译服务配置如下：
  - APIKEY: `ollama`
  - 模型：请见[模型库](https://ollama.com/library)中模型的具体 Tags，例如`qwen:14b-chat-v1.5-q4_1`
  - 自定义 API 接口地址：`http://localhost:11434/v1/chat/completions`；如果你是在局域网内其他主机运行的 ollama 服务，那么请将`localhost`替换为你的主机 IP 地址。
  - 并发速率按照运行的主机算力和使用的模型自行斟酌。小马拉大车也拉不动，配置高了也没有意义。
- 参考文档
  - https://github.com/ollama/ollama/blob/main/docs/api.md
  - https://github.com/ollama/ollama/issues/2335

对于使用 LM-Studio 部署的，可以参考其[文档](https://lmstudio.ai/docs/local-server)，配置方式类似，不过你需要先下载好模型并运行。

### Groq 官方平台

- APIKEY: 到这个[页面](https://console.groq.com/keys)获取密钥。
- 模型：截止到本文撰写时，有四款模型：`llama3-8b-8192`、`llama3-70b-8192`、`mixtral-8x7b-32768`、`gemma-7b-it`，请根据自己的翻译需求测试选择。目前这些模型的中译英效果尚可，但是英译中效果不佳，不建议用于英译中场景。
- 自定义 API 接口地址：`https://api.groq.com/openai/v1/chat/completions`，参见[文档](https://console.groq.com/docs/api-reference#chat-create)
- 速率控制：你可以在这个[页面](https://console.groq.com/settings/limits)查看你账户的请求限速。如果选择的模型的 REQUESTS PER MINUTE 是 30，那么建议设置每秒最大请求数为 1 或 2，不要过高。
- 注意
  - 建议仅在鼠标悬停和输入增强功能处使用

### DeepSeek 官方平台

- APIKEY: 到这个[页面](https://platform.deepseek.com/api_keys)获取密钥。
- 模型：截止到本文撰写时，只推荐该平台的`deepseek-chat`模型用于翻译。
- 自定义 API 接口地址：`https://api.deepseek.com/chat/completions`

### OpenRouter 中转平台

- APIKEY: 到这个[页面](https://openrouter.ai/keys)获取密钥。
- 模型：到这个模型[页面](https://openrouter.ai/docs#models)查看模型列表。例如`anthropic/claude-3-haiku`.
- 自定义 API 接口地址：`https://openrouter.ai/api/v1/chat/completions`
- 限速：请参考[这里](https://openrouter.ai/docs#limits)。截止到本文撰写时，如果你账号里余额是 10 美元，那么你每秒可以发出 10 次请求，20 美元则是 20QPS，依次类推。尽管并发可以很高，但由于平台也是租用官方平台的资源，共享一个大的限速池，所以如果同时使用的人的请求数较多，也会造成请求失败，这种情况并非 OpenRouter 平台的限制，这种请求失败的 HTTP 响应码是 200，但是返回的 Payload 是限速错误说明，沉浸式翻译表现为不显示译文（即返回文本无法解析下，译文为空），这种情况下插件暂时没做相应的空译文异常处理，也不方便重试，遇到这种情况只能更换翻译服务重新翻译。当然，你也可以自建 API 中转处理这种情况。

### 其他

其他平台大同小异，无非是获取 APIKEY、模型名、请求地址，注意速率限制等信息。
